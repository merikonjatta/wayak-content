<!-- vim: set shiftwidth=4 wrap filetype=mediawiki: -->
{{#set: title=第6章: コールバックとパイプ}}
{{Nav
|prev=PerlHard/Chapter5
}}

= 第6章: コールバックとパイプ =

この章では2つのプログラムを実装する。ひとつはHTTPを使ってWebサイトからファイルをダウンロードするget.pl、もうひとつはURIを引数に取りget.plを使ってそれをダウンロードするmirror.plだ。get.plがリンクを発見し、mirror.plがダウンロードする。

これらのプログラムでコールバックという仕組みを理解しよう。コールバックはモジュール内の既存のコードとうまく連携を取る方法の一つだ。また、パイプという仕組みを使えば簡単なプロセス間通信が実現できる。途中、Perlのモジュールをいくつか（URI, HTTP::Request, HTML::Parser, IO::Selectなど）紹介する。

== 6.1 URI ==

Perlはその膨大なモジュールライブラリで有名である。これらのモジュールはほとんどがCPAN（Comprehensive Perl Archive Network, http://cpan.org/）にて入手可能だ。

当然ながらよく使われるモジュールもあれば、そうでもないものもある。一部のモジュールはほとんど言語の一部と言ってしまってもいいほどで、だいたいのPerlインストールパッケージに含まれる。

インターフェースはモジュールによって違うが、この章で扱うものはオブジェクト指向寄りだ。逆にPOSIXモジュールなどはサブルーチンの集まりである。

ウォーミングアップのためにURIという簡単なクラスを使う。URIはほぼURL（インターネットアドレス）と同義だと思ってもらっていい。

このプログラムはURLを文字列として受け取り、URIオブジェクトを作って、Canonicalな形にする。

    use URI;

    sub main {
        my $uris = shift || "http://www.slashdot.com";
        my $uri = URI->new($uris)->canonical;
        $uris = $uri->as_string;
        print "$uris\n";
    }

    main @ARGV;

mainの最初の行はコマンドライン引数のリストから引数を得る。空ならhttp://slashdot.com/ を使う。次にURIオブジェクトを作り、<code>canonical</code>メソッドを呼び出す。このメソッドはURIから非標準な大文字や冗長なポート番号などを削ってくれる。たとえば
    $ ./uri.pl HTTP://www.Slashdot.org:80/Index.html
    http://www.slashdot.org/Index.html
HTTPをhttpにしてホスト名を全部小文字にしてくれた。ファイル名はcase-sensitiveなのでそのままである。ポート番号80は、Webサーバの標準ポート番号が80であるため冗長で、削られた。

他にもURIクラスはURIの部分を取り出すメソッドなども提供する。
    my $scheme = $uri->scheme;
    my $authority = $uri->authority;
    my $path = $uri->path;
schemeはhttpやftpのようなプロトコル名、authoritiyは大抵ホスト名だ。


== 6.2 HTTP GET ==

ファイルをWebサーバから取得するには、クライアントはHTTP GETリクエストを送信しレスポンスを待つことになる。Webクライアントの実装を非常に簡単にしてくれる一連のツールがLWP (World-Wide Web Library for Perl）に用意されている。

ここで使う3つのコンポーネントは<code>UserAgent</code>、<code>Request</code>、<code>Response</code>だ。
    use LWP::UserAgent;
    use HTTP::Request;
    use HTTP::Response;

Requestオブジェクトはクライアントからサーバへのリクエストメッセージ、Responseオブジェクトはサーバからの返答を表す。UserAgentにはこれらのResponseやRequestを送受信する機能が備わっている。簡単なHTTP GETリクエストの例を示すと
    my $ua = LWP::UserAgent->new;
    my $request = HTTP::Request->new(GET => $uri);
    my $response = $ua->request($request);
    print $response->content;
最初の行ではUserAgentを作成する。次にRequestオブジェクトを作成し、この時にURLと、メソッドが（HEADやPOSTではなく）GETであることを指定する。

UserAgentにリクエストを処理させる。するとResponseオブジェクトが返ってくる。この中にヘッダーとコンテンツが含まれるので、それを抽出して出力する。

これらのクラスについて詳しくはLWPドキュメンテーション( http://search.cpan.org/dist/libwww-perl/ )を参照されたい。


== 6.3 コールバック ==

前節ではUserAgentクラスにリクエストを送信させ、結果全体が返ってきてからローカルのファイルに書き込んだ。だが外部ネットワークに依存する処理は遅かったり、予測不能であったりするため、最初のパケットが到着し次第ファイルに書きこむようにしたほうがよい。

requestメソッドには第二引数としてサブルーチンが渡せる。するとデータが到着し次第、そのサブルーチンを呼び出してくれる。引数にはファイルの断片が渡される。

このように引数として渡され、受け手から逆に呼び出されるサブルーチンをコールバックと呼ぶ。例えば、
    sub callback {
        my $chunk = shift;
        print FILE $chunk;
    }
そしてこのようにrequestを呼び出す。
    open FILE, ">", $file or croak "Couldn’t open $file";
    my $request = HTTP::Request->new(GET => $uri);
    my $response = $ua->request($request, \&callback);

=== 練習6.1 ===

この節で紹介したコードをget.plというプログラムにまとめよう。このプログラムはファイルをダウンロードしてコールバックを使ってデータが到着し次第ファイルに書き込む。callbackには、受け取ったデータ断片の長さを出力する行を追加しよう。データ断片のサイズになんらかのパターンがあらわれるだろうか？


== 6.4 ミラーリング ==

次のステップは、Webページをミラーリングすることだ。すなわち、リモートのファイルやディレクトリと同じ名前と構成を持つファイルやディレクトリをローカルに作成する。

そのためにまず、必要ならばディレクトリを作成し、ファイルを開くようにする。URIのパス名から、<code>File::Basename</code>パッケージの<code>dirname</code>メソッドを使って親ディレクトリ名を判定する。
    my $path = $uri->path;
    my $dir = $uri->authority . File::Basename::dirname $path;
    \end{verbatim}
続いてディレクトリを作成する。
    \begin{verbatim}
    my $res = system "mkdir -p $dir";
    if ($res != 0) { croak "Couldn’t create directory $dir" };
system演算子はサブプロセスを作成してコマンドを実行してくれる。ここではUNIXでディレクトリを作成するmkdirコマンドを呼び出している。 -pオプションは深いディレクトリも一気に作成するオプションだ。

残念ながらUNIXコマンドの返却値はPerlの慣例とは違っている。Perlではtrueな値は成功を意味し、falseな値は失敗を意味するが、UNIXでは0が成功で他はエラーだ。

最後にファイル名を構築して開く。
    my $file = $uri->authority . $path;
    open FILE, ">", $file or croak "Couldn’t open $file";
あるサイトからダウンロードしたファイルは全て同じディレクトリ以下に入るよう、親ディレクトリの名前はURIのauthorityパートを使う。

=== 練習6.2 ===

get.plを変更し、ファイルをダウンロードした際に元サイトの構成をミラーリングするようにしよう。

=== 練習6.3 ===

実はHTTPの仕様によってこの部分は少し厄介になる。URIがディレクトリを指すとき、サーバはindex.htmlというファイルを探してそれを送信してくるのだ。残念ながらURIを見ただけでそれがファイルなのかディレクトリなのかは判別できない。

一つの解決策として、URIのパスが.htmlで終わるかどうかを見ればいい。違ったらパスに/index.htmlを追記する。だがこれはいつもうまくいくとは限らない。練習として何らかの解決策を模索してみよう。

== 6.5 パース ==

Webページのコンテンツをダウンロードすると同時に、リンクも抽出したい。そのためには正規表現で&lt;a&gt;タグを探してもいいが、HTML::Parserモジュールを使えば簡単だ。

まずはParserオブジェクトを作成する。
    our $p = HTML::Parser->new(start_h => [\&start_tag, "tagname, attr"]);
引数はいつもどおり、インスタンス変数の名前と値のリストである。ここではstart_hというインスタンス変数を指定する。これにはパーサが開始タグを見つけた時に呼び出されるハンドラを指定する。ハンドラーとは、何らかのイベントが起こった時に呼び出されるコールバックのことだ。Parserには他にも終了タグで発生するend_hやタグの一部でないプレーンテキストで発生するtext_hといったイベントがある。

start_hにはリストを与える。これにはコールバックへの参照 (start_tag)に加え、コールバックに与えられるべき引数のリストが含まれる。この場合、タグの名前と属性のハッシュが欲しい。

たとえば次のようなリンクタグがあったとする。
    <a href="http://allendowney.com">my web page</a>
このコードがパースされると、3つのイベントが発生する。まずは開始タグで、タグ名はa、属性ハッシュには<code>href =&gt; http://allendowney.com</code>という一つの要素が含まれる。次はテキストイベントで、<code>my web page</code>という文字列がある。最後は終了タグで、タグ名はa、属性はなしである。

aタグを待ってリンク先のURIを出力するstart_tagの例はこうなる。
    sub start_tag {
        my ($tagname, $attr) = @_;
        if ($tagname eq "a") {
            my $href = $attr->{href};
            print "$href\n";
        }
    }
Parserオブジェクトはインクリメンタルだ。すなわち、別々のHTML断片で何回でも繰り返して呼び出せる。この仕様は先程の到着し次第ファイルに書き出すコードと相性が良い。そこでcallbakを変更し、ファイルに書きこむと同時にパーサに渡すようにする。
    sub callback {
        my $line = shift;
        our $p->parse($line);
        print FILE $line;
    }
コールバックを使うと様々なやり方でモジュールをプログラムに組み込むことができる。が、最大の欠点はプログラムのフローが複雑になりがちな点だ。

この場合だとファイルの断片が到着するとUserAgentはcallbackを呼び出し、それがParserへ断片を渡し、もしそこに開始タグが含まれていればそこからstart_tagが呼び出される。


== 6.6 絶対URIと相対URI ==

ここまでみてきたURIは絶対URIだ。すなわち、httpなどのスキームから始まってホスト名とパスが含まれていた。だが&lt;a&gt;タグにはパスしか含まれない相対URLがあるかもしれない。

例えばhttp://allendowney.com/index.htmlはファイルの絶対URIだ。このファイルに相対URLが含まれていたら、親ディレクトリから相対的に見たファイルを指すことになる。projects.htmlへのリンクはhttp://allendowney.com/projects.htmlを指す。基準となる親ディレクトリはベースURIと呼ばれる。

URIクラスには相対URIを絶対URIに変換するabsメソッドがある。
    my $base = URI->new("http://allendowney.com/index.html");
    my $rel = URI->new("projects.html");
    my $abs = $rel->abs($base);
    print "$abs\n";
出力は http://allendowney.com/projects.html だ。

=== 練習6.4 ===

start_tagを変更し、相対URIを見つけたら現在のページを基準に絶対URIに変換するようにしよう。

=== 練習6.5 ===

start_tagを変更し、内部リンクだけを出力するようにしよう。つまり、今のファイルとホスト名が同じURIだけにしよう。


== 6.7 複数のプロセス ==

現時点で我々のプログラムは、与えられたURIをダウンロードし、結果を適切なファイル名に保存し、含まれるリンクのURIを出力してくれる。これをつかってボット、つまりWebサイトの全ファイルをリンクをたどりながらダウンロードするプログラムを作ろうと思う。

こういったプログラムはマルチスレッド化して複数のスレッドを同時実行させたほうがよい。理由は
* ネットワークはローカル処理と比べて遅いので、いくつかの接続を同時に行ったほうが良い
* ネットワークのパフォーマンスは変動しやすい。ある接続が停止してしまっていてもその間に別の接続ができる。

もちろん、複数の接続を同時に立てるとネットワークとサーバのリソースを奪い合うので、多けりゃいいってもんでもない。

== 6.8 家族計画 ==

ここからは子プロセスを作成してファイルをダウンロードする親プログラムを作成する。親プログラムはひとつのURIから始めて、そのファイルをダウンロードする子プロセスをひとつ生成する。子プロセスがリンクを見つけるたびに、親プログラムは追加の子プロセスを生成していく。親プログラムの役割はどう実行する子プロセスの数を管理し、ダウンロード待ちのリンクのリストを保持することだ。

親プログラムのメインループはこのようになる。
    sub main {
        our @list = @_;
        our %seen;
        our $procs = 0;
        our $s = IO::Select->new();
        while (1) {
            while (@list > 0 && $procs < 6) {
                get shift @list;
            }
            my @ready = $s->can_read;
            foreach my $fh (@ready) {
                process $fh;
            }
            last if (@list == 0 && $procs == 0);
        }
    }

@listはダウンロード待ちのURIのリストで、最初はコマンドライン引数から得る。%seenは既にダウンロードしたURIを保持するハッシュだ。URIを@listに加える前に、既に%seenに入っていないかを確認する。$procsは現在実行中の子プロセスの数だ。

$sはIO::Selectオブジェクトで、selectシステムコールへの簡便なインターフェースを提供してくれる。メインループの真ん中あたりでこれに対してcan_readを呼び出し、読み込めるデータがあるファイルハンドルのリスト、つまりリンクを見つけた子プロセスのリストを得る。

メインループの中には2つのループがある。一つ目はプロセスの数を制御している。2つ目は子プロセスからリンクを集めてリストに追加する。

@listが空になり、子プロセスもなくなるとループは終了する。


== 6.9 子作り ==

メインループでは毎回、@listをチェックして最大で6の子プロセスを立てる。リストの各要素に関してgetを呼び出す。
    sub get {
        our (%seen, $procs, $s);
        my $uris = shift;

        open my $fh, "./get.pl $uris |" or croak "can’t fork";
        $seen{$uris}++;
        $procs++;
        $s->add($fh);
    }
ここでopenを使ってパイプを作成している。パイプとは、ファイルハンドルを通じて親プロセスとやり取りする子プロセスのことだ。この場合、子プロセスはprint演算子で出力を行い、親プロセスはファイルハンドルでそれを読み取る。

open演算子が成功したら%seenと$procsを更新し、ファイルハンドル$fhをSelectオブジェクトの監視下に置く。メインループでcan_readを呼び出すと読み込めるファイルハンドルがどれかを返してくれる。


== 6.10 親に伝言 ==

can_readはどれかのファイルハンドルの準備ができるまで返らないが、返ったら、ファイルハンドルのリストを走査し、それぞれにprocessを呼び出す。
    sub process {
        our (@list, %seen, $procs, $s);
        my $fh = shift;
        my $line = <$fh>;
        chomp $line;

        if ($line eq "done") {
            close $fh;
            $s->remove($fh);
            $procs--;
        } else {
            push @list, $line unless defined $seen{$line};
        }
    }
processでは山括弧演算子を使ってファイルハンドルから一行を読み出し、chompで末尾の改行を削る。

selectと山括弧演算子を同時につかうのはまずい場面もあるが、今回は子プロセスがデータを吐くときは必ず行単位であることが分かっているので問題ない。そうでない場合は改行が出力されるまで山括弧演算子がブロックしてしまうことがある。

行を読み込んでからは2つの場合がある。読み込んだ行が"done"であれば子プロセスは終了したということなのでファイルハンドルを閉じ、Selectオブジェクトの管理下から外し、子プロセスの数をデクリメントする。読み込んだ行がリンクなら、ダウンロード待ちリストに入れる（%seenに入っていない場合は）。


=== 練習6.6 ===

個々までのプログラムをmirror.plというプログラムにまとめ、どこかのサイトで試してみよう。ただしこのプログラムは非常に強引なので気をつけよう。サーバに負荷をかけすぎるかもしれない。ボットとしてのマナーについてより詳しくはLWP::RobotUAのドキュメンテーションなどを参照されたい。

== 6.11 練習 ==

=== 練習6.7 ===

Webサイトのローカルコピーを作れたら、それを定期的に更新しておきたいかもしれない。もちろん、毎回全部のファイルをダウンロードしてもいいだろうが、新しく更新されたファイルだけをダウンロードするのが望ましい。

そこでプログラムを書き換え、ダウンロードの前にローカルに存在するかどうかを確かめるようにしよう。もしあれば、ローカルファイルの変更時刻をチェックし、それ以降にリモートファイルが更新されていたときだけダウンロードし直すようにしよう。

ヒント：ローカルファイルの変更時刻はstatコマンドおよびFile::Statモジュールを使えば取得できそうだ。条件付きリクエストについてはHTTP::Headersのドキュメンテーションをチェックしよう。

=== 練習6.8 ===

このプログラムはリダイレクトに対応していない。HTTPリクエストがリダイレクトされると、最初のURIとレスポンスで返ってきたファイルのURIは食い違う。たとえば http://www.slashdot.com をリクエストすると実際には http://slashdot.org/ が返される。

理想としては、mirrorプログラムはResponseオブジェクトのbaseメソッドでファイル名を判断すべきだが、UserAgent::requestのコールバック形態を使っていると、Responseオブジェクトはデータの転送が完了するまで得られない。

そこで、この問題の解決策をいくつか考えてみよう。最も良いと思われるものを実装してみよう。

=== 練習6.9 ===

このmirrorプログラムはWebページの一部へリンクするURIをうまく扱えない。URIモジュールのドキュメンテーションで関連する情報を調べ、問題を解決しよう。

<references/>

{{Nav
|prev=PerlHard/Chapter5
}}
